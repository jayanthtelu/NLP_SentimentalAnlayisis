{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9ff1bf",
   "metadata": {},
   "source": [
    "#### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed0400",
   "metadata": {},
   "source": [
    "To obtain data related to the University of Maryland, College Park, a data extraction process was performed using the tweepy package, which is a Python library for accessing the Twitter API. The extraction was performed by searching for various hashtags related to the University of Maryland, College Park on Twitter. These hashtags were used as search terms to filter the tweets related to the University of Maryland, College Park. The extracted data was then stored in a data structure for further analysis and processing. This process allowed us to collect relevant data related to the University of Maryland, College Park from Twitter.\n",
    "\n",
    "This Python code utilizes the Twitter API and the Tweepy library to collect tweets related to a set of predefined hashtags. The hashtags are stored in a list and the code loops through each hashtag to search for related tweets. The search is limited to English language tweets and only 100 tweets are returned per page with a maximum of 200 pages per hashtag. \n",
    "\n",
    "The code also filters out retweets and duplicates by keeping track of unique tweets using a set data structure. Regular expressions are used to remove hashtags and URLs from the tweets to clean up the text data. \n",
    "\n",
    "The collected tweets are stored in a Pandas DataFrame and exported to a CSV file. The DataFrame includes the tweet content, the hashtag it was collected from, and the timestamp of the tweet. The code stops collecting tweets after 3000 unique tweets have been collected across all hashtags. \n",
    "\n",
    "Overall, this code provides a useful tool for collecting and analyzing tweets related to specific topics or keywords on Twitter. It could be adapted for various research projects and social media monitoring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5804b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set up API keys\n",
    "consumer_key = MASKED\n",
    "consumer_secret = MASKED\n",
    "access_token = MASKED\n",
    "access_token_secret = MASKED\n",
    "\n",
    "# Authenticate with Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "hashtags = [\"umd\",\"umcp\", \"FearlesslyUMD\", \"CollegePark\",\"smithterps\", \"MarylandDay\", \"IAmARHU\", \"UMDgrad\", \"AGNRTerps\", \"GivingDayUMD\", \"DoGoodUMD\"]\n",
    "unique_tweets = set()\n",
    "tweet_list = []\n",
    "counter = 0\n",
    "for hashtag in hashtags:\n",
    "        query = hashtag\n",
    "        # Loop through search results using pagination\n",
    "        for page in tweepy.Cursor(api.search_tweets, q=hashtag, lang='en', tweet_mode='extended',count =100).pages(200):\n",
    "        # Loop through tweets on the page\n",
    "            for tweet in page:\n",
    "            # Check if the tweet is a retweet or a duplicate\n",
    "                if 'retweeted_status' in dir(tweet):\n",
    "                    tweet_text = tweet.retweeted_status.full_text\n",
    "                else:\n",
    "                    tweet_text = tweet.full_text\n",
    "                    \n",
    "                tweet_text = re.sub(r\"(#[\\d\\w]+)|(http\\S+)\", \"\", tweet_text).strip()\n",
    "                \n",
    "                if tweet_text not in unique_tweets:\n",
    "                    unique_tweets.add(tweet_text)\n",
    "                \n",
    "                    tweet_list.append({\n",
    "                        'content': tweet_text, \n",
    "                        'hashtag': hashtag, \n",
    "                        'timestamp': tweet.created_at})\n",
    "                    counter += 1\n",
    "                if counter == 6000:\n",
    "                    break\n",
    "                \n",
    "        if counter == 6000:\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(tweet_list)\n",
    "df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29a308",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3179",
   "metadata": {},
   "source": [
    "This Python code is designed to preprocess the text of tweets collected from the Twitter API using the Tweepy library. The code makes use of the Natural Language Toolkit (NLTK), a popular library for working with natural language data in Python.\n",
    "\n",
    "The code begins by downloading the NLTK resources for stop words and tokenization. Stop words are common words like \"the\", \"and\", and \"is\" that are often removed from text data because they do not add significant meaning. Tokenization is the process of splitting text into individual words or tokens.\n",
    "\n",
    "The code then loads the stop words and punctuation from the NLTK resources into Python sets. These sets will be used later to remove stop words and punctuation from the tweet text.\n",
    "\n",
    "Next, the code defines a function called \"preprocess_tweet\" that takes a single tweet as input and returns a cleaned version of the text. The function performs several steps to clean the tweet text:\n",
    "\n",
    "1. Remove URLs and mentions: The regular expression in the code removes any URLs or Twitter handles (mentions) from the tweet text using the re.sub() function.\n",
    "\n",
    "2. Tokenize the tweet text: The word_tokenize() function from NLTK is used to split the text into individual words or tokens.\n",
    "\n",
    "3. Remove stop words and punctuation: The code uses a list comprehension to remove any tokens that are stop words or punctuation marks.\n",
    "\n",
    "4. Join the tokens back into a single string: The cleaned tokens are joined back together into a single string using the str.join() method.\n",
    "\n",
    "The cleaned tweet text is then returned by the function.\n",
    "\n",
    "Overall, this code provides a useful tool for preprocessing text data from Twitter for use in natural language processing (NLP) tasks like sentiment analysis or topic modeling. It could be adapted for various research projects and social media monitoring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c433bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download the NLTK resources for stop words and tokenization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Define a function to preprocess the tweet text\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs and mentions\n",
    "    tweet = re.sub(r\"(#[\\d\\w]+)|(http\\S+)|(@\\w+)\", \"\", tweet)\n",
    "    # Tokenize the tweet text\n",
    "    tokens = word_tokenize(tweet)\n",
    "    # Remove stop words and punctuation from the tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_tweet = \" \".join(tokens)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469014ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tweet content in the DataFrame\n",
    "df = pd.read_csv('tweets.csv')\n",
    "df['content'] = df['content'].apply(preprocess_tweet)\n",
    "df.dropna(subset=['content'], inplace=True)\n",
    "df.to_csv('cleaned_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e311e8",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a638f6f",
   "metadata": {},
   "source": [
    "This code uses scikit-learn's CountVectorizer and TfidfVectorizer to convert the preprocessed tweet text into a matrix of word counts and TF-IDF values, respectively. \n",
    "\n",
    "The CountVectorizer counts the occurrences of each word in the tweet text and creates a matrix where each row corresponds to a tweet and each column corresponds to a word in the vocabulary. The value in each cell of the matrix represents the number of times the corresponding word appears in the corresponding tweet. \n",
    "\n",
    "The TfidfVectorizer weighs the importance of each word in the tweet text by calculating the term frequency-inverse document frequency (TF-IDF) score. The TF-IDF score for a word in a tweet is calculated as the product of its term frequency (the number of times it appears in the tweet) and its inverse document frequency (the logarithm of the total number of tweets divided by the number of tweets that contain the word). The resulting matrix has the same format as the count matrix, with rows corresponding to tweets and columns corresponding to words.\n",
    "\n",
    "Finally, the code converts the count and TF-IDF matrices into Pandas DataFrames for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98529b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayan\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\jayan\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words DataFrame:\n",
      "    00  000  00_ethers  00am  00pm  01  02  03  0301  04  ...  ùòÅùóµùó∂ùòÄ  ùòÜùó≤ùóÆùóø  \\\n",
      "0   0    0          0     0     0   0   0   0     0   0  ...     0     0   \n",
      "1   0    0          0     0     0   0   0   0     0   0  ...     0     0   \n",
      "2   0    0          0     0     0   0   0   0     0   0  ...     0     0   \n",
      "3   0    0          0     0     0   0   0   0     0   0  ...     0     0   \n",
      "4   0    0          0     0     0   0   0   0     0   0  ...     0     0   \n",
      "\n",
      "   ùôÇùôöùô©ùô©ùôûùô£ùôú  ùôàùôûùôôùô¨ùôöùôöùô†  ùôçùôöùôñùôôùôÆ  ùôòùô°ùôñùô®ùô®ùôßùô§ùô§ùô¢  ùôôùô§ùô£ùôö  ùôûùô£  ùôüùô§ùôó  ùô©ùôùùôö  \n",
      "0        0        0      0          0     0   0    0    0  \n",
      "1        0        0      0          0     0   0    0    0  \n",
      "2        0        0      0          0     0   0    0    0  \n",
      "3        0        0      0          0     0   0    0    0  \n",
      "4        0        0      0          0     0   0    0    0  \n",
      "\n",
      "[5 rows x 9427 columns]\n",
      "\n",
      "TF-IDF DataFrame:\n",
      "     00  000  00_ethers  00am  00pm   01   02   03  0301   04  ...  ùòÅùóµùó∂ùòÄ  ùòÜùó≤ùóÆùóø  \\\n",
      "0  0.0  0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0  0.0  ...   0.0   0.0   \n",
      "1  0.0  0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0  0.0  ...   0.0   0.0   \n",
      "2  0.0  0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0  0.0  ...   0.0   0.0   \n",
      "3  0.0  0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0  0.0  ...   0.0   0.0   \n",
      "4  0.0  0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0  0.0  ...   0.0   0.0   \n",
      "\n",
      "   ùôÇùôöùô©ùô©ùôûùô£ùôú  ùôàùôûùôôùô¨ùôöùôöùô†  ùôçùôöùôñùôôùôÆ  ùôòùô°ùôñùô®ùô®ùôßùô§ùô§ùô¢  ùôôùô§ùô£ùôö   ùôûùô£  ùôüùô§ùôó  ùô©ùôùùôö  \n",
      "0      0.0      0.0    0.0        0.0   0.0  0.0  0.0  0.0  \n",
      "1      0.0      0.0    0.0        0.0   0.0  0.0  0.0  0.0  \n",
      "2      0.0      0.0    0.0        0.0   0.0  0.0  0.0  0.0  \n",
      "3      0.0      0.0    0.0        0.0   0.0  0.0  0.0  0.0  \n",
      "4      0.0      0.0    0.0        0.0   0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 9427 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df=pd.read_csv('cleaned_tweets.csv')\n",
    "df = df.dropna(subset=['content'])\n",
    "# Create a CountVectorizer to count the occurrences of each word in the tweet text\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Create a TfidfVectorizer to weigh the importance of each word in the tweet text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizers to the preprocessed tweet text\n",
    "count_matrix = count_vectorizer.fit_transform(df['content'])\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['content'])\n",
    "\n",
    "# Convert the matrix to a DataFrame\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Bag-of-words DataFrame:\\n\", count_df.head())\n",
    "print(\"\\nTF-IDF DataFrame:\\n\", tfidf_df.head())\n",
    "\n",
    "count_df.to_csv('count_df_tweets.csv', index=False)\n",
    "tfidf_df.to_csv('tfidf_df_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda59f02",
   "metadata": {},
   "source": [
    "#### Labelling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300defa5",
   "metadata": {},
   "source": [
    "This code uses the Hugging Face Transformers library to perform sentiment analysis on a list of tweets. The sentiment analysis is done using a pre-trained model, which is accessed through the `pipeline()` function. \n",
    "\n",
    "The `pipeline()` function is called with the argument `'sentiment-analysis'` to specify that we want to perform sentiment analysis. \n",
    "\n",
    "The tweets are first extracted from the `df` DataFrame and stored in a list. \n",
    "\n",
    "The `classifier()` function is then called on the list of tweets, which returns a list of dictionaries containing the sentiment label and score for each tweet. \n",
    "\n",
    "The sentiment labels are extracted from the list of dictionaries and stored in a new DataFrame `df_sentiments` along with the original tweet text. \n",
    "\n",
    "Finally, the `head()` function is used to display the first few rows of the `df_sentiments` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7c3764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content sentiment\n",
      "0                  Had great time today Thank invite  POSITIVE\n",
      "1   Had amazing junior day Thank coaches hospitality  POSITIVE\n",
      "2  As Lucy Dalglish approaches conclusion 11-year...  POSITIVE\n",
      "3  Kettle Moraine 2024 DB speedster Noah Hait exc...  POSITIVE\n",
      "4                               UMD HERE WE COME ü•≥ü•≥üéâ  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "tweets = df['content'].tolist()\n",
    "\n",
    "results = classifier(tweets)\n",
    "\n",
    "sentiments = []\n",
    "for result in results:\n",
    "    sentiment = result['label']\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "df_sentiments = pd.DataFrame({'content': tweets, 'sentiment': sentiments})\n",
    "print(df_sentiments.head())\n",
    "df_sentiments.to_csv('tweets_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225b086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('tweets_sentiment.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    with open('preprocessed_tweets.txt', 'w', encoding='utf-8') as txtfile:\n",
    "        for row in reader:\n",
    "            txtfile.write('\\t'.join(row) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c95396",
   "metadata": {},
   "source": [
    "##### Model Training & Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e081d0",
   "metadata": {},
   "source": [
    "This code trains a deep learning model to classify tweets as positive or negative based on their content. The code first reads in preprocessed tweet data and converts the labels to binary values. Then, it tokenizes the text using white space tokenization and pads the sequences of the text. The data is split into training and testing sets. \n",
    "\n",
    "The model architecture consists of an embedding layer, a convolutional layer, a max pooling layer, a long short-term memory (LSTM) layer, a reshape layer, a gated recurrent unit (GRU) layer, and a dense layer with a sigmoid activation function. The model is compiled using binary cross-entropy as the loss function and Adam as the optimizer. The model is trained on the training data with 25 epochs and a batch size of 128. \n",
    "\n",
    "The code evaluates the model on the test data and calculates the accuracy, precision, recall, and F1-score. It also outputs a classification report and a confusion matrix. The confusion matrix is displayed using seaborn heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d3a3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - 21s 615ms/step - loss: 0.6934 - accuracy: 0.5208 - val_loss: 0.6951 - val_accuracy: 0.4847\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 15s 694ms/step - loss: 0.6938 - accuracy: 0.4872 - val_loss: 0.6937 - val_accuracy: 0.4847\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 14s 622ms/step - loss: 0.6935 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.4861\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 16s 715ms/step - loss: 0.6929 - accuracy: 0.5036 - val_loss: 0.6849 - val_accuracy: 0.6555\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 16s 702ms/step - loss: 0.5455 - accuracy: 0.7376 - val_loss: 0.5337 - val_accuracy: 0.7387\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 16s 749ms/step - loss: 0.2279 - accuracy: 0.9281 - val_loss: 0.7089 - val_accuracy: 0.7650\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 16s 728ms/step - loss: 0.0880 - accuracy: 0.9715 - val_loss: 0.8482 - val_accuracy: 0.7810\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 17s 804ms/step - loss: 0.0401 - accuracy: 0.9894 - val_loss: 0.8905 - val_accuracy: 0.7912\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 16s 739ms/step - loss: 0.0242 - accuracy: 0.9934 - val_loss: 0.9814 - val_accuracy: 0.7810\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 15s 699ms/step - loss: 0.0132 - accuracy: 0.9978 - val_loss: 1.1840 - val_accuracy: 0.7839\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 16s 711ms/step - loss: 0.0179 - accuracy: 0.9949 - val_loss: 1.1485 - val_accuracy: 0.7985\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 16s 719ms/step - loss: 0.0332 - accuracy: 0.9945 - val_loss: 0.7506 - val_accuracy: 0.7693\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.0249 - accuracy: 0.9949 - val_loss: 1.1605 - val_accuracy: 0.7708\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 17s 759ms/step - loss: 0.0182 - accuracy: 0.9967 - val_loss: 0.9614 - val_accuracy: 0.7810\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 16s 732ms/step - loss: 0.0140 - accuracy: 0.9978 - val_loss: 1.1584 - val_accuracy: 0.7620\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 17s 754ms/step - loss: 0.0147 - accuracy: 0.9971 - val_loss: 1.1688 - val_accuracy: 0.7883\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 16s 743ms/step - loss: 0.0121 - accuracy: 0.9982 - val_loss: 1.1643 - val_accuracy: 0.7898\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 17s 792ms/step - loss: 0.0144 - accuracy: 0.9978 - val_loss: 1.0630 - val_accuracy: 0.7839\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 17s 762ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 1.0962 - val_accuracy: 0.7854\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 16s 711ms/step - loss: 0.0119 - accuracy: 0.9982 - val_loss: 1.2103 - val_accuracy: 0.7825\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 16s 730ms/step - loss: 0.0115 - accuracy: 0.9982 - val_loss: 1.3733 - val_accuracy: 0.7825\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 16s 721ms/step - loss: 0.0194 - accuracy: 0.9967 - val_loss: 1.2250 - val_accuracy: 0.7766\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 17s 793ms/step - loss: 0.0218 - accuracy: 0.9964 - val_loss: 1.2058 - val_accuracy: 0.7766\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 16s 745ms/step - loss: 0.0176 - accuracy: 0.9964 - val_loss: 1.1317 - val_accuracy: 0.7883\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 18s 832ms/step - loss: 0.0192 - accuracy: 0.9960 - val_loss: 0.8879 - val_accuracy: 0.7796\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 18s 810ms/step - loss: 0.0168 - accuracy: 0.9971 - val_loss: 1.0467 - val_accuracy: 0.7927\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 18s 842ms/step - loss: 0.0139 - accuracy: 0.9974 - val_loss: 1.0914 - val_accuracy: 0.7898\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 18s 837ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 1.1749 - val_accuracy: 0.7883\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 18s 825ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 1.2026 - val_accuracy: 0.7927\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 18s 843ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 1.2710 - val_accuracy: 0.7927\n",
      "22/22 [==============================] - 3s 79ms/step\n",
      "Accuracy: 0.7927007299270074\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.8012048192771084\n",
      "F1-Score: 0.7893175074183976\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.80       353\n",
      "           1       0.78      0.80      0.79       332\n",
      "\n",
      "    accuracy                           0.79       685\n",
      "   macro avg       0.79      0.79      0.79       685\n",
      "weighted avg       0.79      0.79      0.79       685\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPooling1D, GRU\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "with open('preprocessed_tweets.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    preprocessed_tweet_text = []\n",
    "    labels = []\n",
    "    for line in f:\n",
    "        text, label = line.strip().split('\\t')\n",
    "        preprocessed_tweet_text.append(text)\n",
    "        labels.append(label.lower())\n",
    "\n",
    "# Convert the labels to binary values\n",
    "labels = np.array([1 if label == 'positive' else 0 for label in labels])\n",
    "\n",
    "# Tokenize the text using white space tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_tweet_text)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_tweet_text)\n",
    "\n",
    "# Pad the sequences of the text\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(Reshape((-1, 128)))\n",
    "model.add(GRU(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,  y_train, validation_data=(X_test, y_test), epochs=30, batch_size=128)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics and confusion matrix\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-Score:', f1)\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c519d6",
   "metadata": {},
   "source": [
    "#### Step 6: Predicting the sentiment of the 10 Random Sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33b6a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'I am so happy to be spending time with my family today.',\n",
    "    'The loss of my dog has left me feeling devastated.',\n",
    "    'This new job opportunity has me feeling excited and nervous at the same time.',\n",
    "    'Seeing my best friend after a long time made me feel overjoyed.',\n",
    "    'I feel so grateful for the support and encouragement I\\'ve received from my loved ones.',\n",
    "    'The news of my grandmother\\'s passing has left me feeling heartbroken.',\n",
    "    'I\\'m feeling a mix of anticipation and anxiety as I prepare for my upcoming presentation.',\n",
    "    'Spending time alone in nature always leaves me feeling peaceful and refreshed.',\n",
    "    'The unexpected act of kindness from a stranger made my day.',\n",
    "    'The constant stress and pressure from work has left me feeling overwhelmed and exhausted.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b116480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "Sentence: I am so happy to be spending time with my family today.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The loss of my dog has left me feeling devastated.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Sentence: This new job opportunity has me feeling excited and nervous at the same time.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Sentence: Seeing my best friend after a long time made me feel overjoyed.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Sentence: I feel so grateful for the support and encouragement I've received from my loved ones.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Sentence: The news of my grandmother's passing has left me feeling heartbroken.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Sentence: I'm feeling a mix of anticipation and anxiety as I prepare for my upcoming presentation.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Sentence: Spending time alone in nature always leaves me feeling peaceful and refreshed.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Sentence: The unexpected act of kindness from a stranger made my day.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Sentence: The constant stress and pressure from work has left me feeling overwhelmed and exhausted.\n",
      "Predicted Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to preprocess a given sentence\n",
    "def preprocess_sentence(sentence):\n",
    "    words = sentence.lower().split()  # convert to lowercase and split into words\n",
    "    words = [word for word in words if word not in stop_words]  # remove stopwords\n",
    "    return \" \".join(words)  # join the words back into a sentence\n",
    "\n",
    "# Define a function to predict sentiment for a given sentence\n",
    "def predict_sentiment(sentence):\n",
    "    cleaned_sentence = preprocess_sentence(sentence)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)[0]\n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    return sentiment\n",
    "\n",
    "# Predict sentiments for all test sentences and print the results\n",
    "for sentence in test_sentences:\n",
    "    sentiment = predict_sentiment(sentence)\n",
    "    print(f\"Sentence: {sentence}\\nPredicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa49a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sentence: The weather outside is beautiful and perfect for a picnic.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Sentence: I had a terrible day at work today.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Sentence: I just finished reading a great book and it was amazing.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Sentence: The traffic today was horrible and it made me late for my appointment.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Sentence: I love spending time with my family and friends.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Sentence: The food at this restaurant is always delicious.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Sentence: I can't believe I forgot my phone at home today.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Sentence: I feel so lucky to have such wonderful people in my life.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Sentence: The movie I watched last night was boring and predictable.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Sentence: I'm so excited to start my new job next week!\n",
      "Predicted Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess the text and generate padded sequences\n",
    "def preprocess_sentence(sentence):\n",
    "    words = sentence.lower().split()  # convert to lowercase and split into words\n",
    "    words = [word for word in words if word not in stop_words]  # remove stopwords\n",
    "    return \" \".join(words)  # join the words back into a sentence\n",
    "\n",
    "# Define a function to predict the sentiment of a sentence\n",
    "def predict_sentiment(sentence):\n",
    "    cleaned_sentence = preprocess_sentence(sentence)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)[0]\n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    return sentiment\n",
    "\n",
    "# Test the function with 10 sample sentences\n",
    "sample_texts = [\n",
    "    'The weather outside is beautiful and perfect for a picnic.',\n",
    "    'I had a terrible day at work today.',\n",
    "    'I just finished reading a great book and it was amazing.',\n",
    "    'The traffic today was horrible and it made me late for my appointment.',\n",
    "    'I love spending time with my family and friends.',\n",
    "    'The food at this restaurant is always delicious.',\n",
    "    'I can\\'t believe I forgot my phone at home today.',\n",
    "    'I feel so lucky to have such wonderful people in my life.',\n",
    "    'The movie I watched last night was boring and predictable.',\n",
    "    'I\\'m so excited to start my new job next week!'\n",
    "]\n",
    "\n",
    "for sentence in sample_texts:\n",
    "    sentiment = predict_sentiment(sentence)\n",
    "    print(f\"Sentence: {sentence}\\nPredicted Sentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873aed4",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e45317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
